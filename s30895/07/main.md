ðŸ“Œ Random Forest â€” Performance
__Random Forest performance__ \
confusion matrix:  \
 [[19  0  0]] \
 [[ 0 21  0]] \
 [[ 0  0 14]] \
accuracy: 1.0 \
precision: 1.0 \
f1 score: 1.0 

ðŸ“Œ Model 1 â€” 42kb\
Layer (type)                 Output Shape     Param # \
dense (Dense)                (None, 32)       448     \
activation (Activation)      (None, 32)       0       \
dense_1 (Dense)              (None, 16)       528     \
activation_1 (Activation)    (None, 16)       0       \
dense_2 (Dense)              (None, 3)        51      

confusion matrix: \
 [[19  0  0]] \
 [[ 7 14  0]] \
 [[ 0  0 14]] \
accuracy: 0.8703703703703703 \
precision: 0.9102564102564102 \
f1 score: 0.8814814814814814 

ðŸ“Œ Model 2 â€” Batch Normalization â€” 48kb\
Layer (type)                 Output Shape     Param # \
dense (Dense)                (None, 32)       448     \
activation (Activation)      (None, 32)       0       \
dense_1 (Dense)              (None, 16)       528     \
batch_normalization          (None, 16)       64      \
activation_1 (Activation)    (None, 16)       0       \
dense_2 (Dense)              (None, 3)        51      

confusion matrix: \
 [[18  1  0]] \
 [[ 11 10  0]] \
 [[ 0  5 9]] \
accuracy: 0.7259259259259259 \
precision: 0.7466666666666667 \
f1 score: 0.7220054837446141 \
batch normalization pogorszyÅ‚o wyniki koÅ„cowe oraz wprowadziÅ‚o ogromnÄ… \
niestabilnoÅ›Ä‡ podczas uczenia â€” validation accuracy wahaÅ‚o siÄ™ miÄ™dzy \
0.6 a 0.95 

ðŸ“Œ Model 3 â€” Layer Normalization â€” 47kb\
Layer (type)                 Output Shape     Param # \
dense (Dense)                (None, 32)       448     \
activation (Activation)      (None, 32)       0       \
dense_1 (Dense)              (None, 16)       528     \
layer_normalization          (None, 16)       32      \
activation_1 (Activation)    (None, 16)       0       \
dense_2 (Dense)              (None, 3)        51      

confusion matrix: \
 [[19  0  0]] \
 [[ 3 18  0]] \
 [[ 0  0 14]] \
accuracy: 0.9444444444444444 \
precision: 0.9545454545454546 \
f1 score: 0.9499687304565354 \
layer normalization daÅ‚o stabilne uczenie i wyniki lepsze w porÃ³wnaniu z modelem bazowym

ðŸ“Œ Model 4 â€” Zmniejszone warstwy (32â†’16, 16â†’8)\
Layer (type)                 Output Shape     Param # \
dense (Dense)                (None, 16)       224     \
activation (Activation)      (None, 16)       0       \
dense_1 (Dense)              (None, 8)        136     \
layer_normalization          (None, 8)        16      \
activation_1 (Activation)    (None, 8)        0       \
dense_2 (Dense)              (None, 3)        27      

confusion matrix: \
 [[18  1  0]] \
 [[ 0 21  0]] \
 [[ 2  1 11]] \
accuracy: 0.9259259259259259 \
precision: 0.9376811594202898 \
f1 score: 0.9192074592074593 \
zmiana rozmiarÃ³w warstw 32â†’16 i 16â†’8 nie wpÅ‚yneÅ‚a znaczÄ…co na wyniki 

ðŸ“Œ Model 5 â€” Minimalny model â€” 36kb\
Layer (type)                 Output Shape     Param # \
dense (Dense)                (None, 8)        112     \
activation (Activation)      (None, 8)        0       \
dense_1 (Dense)              (None, 4)        36      \
layer_normalization          (None, 4)        8       \
activation_1 (Activation)    (None, 4)        0       \
dense_2 (Dense)              (None, 3)        15      

__dnn performance__ \
confusion matrix: \
 [[19  0  0]] \
 [[ 2 17  2]] \
 [[ 0  3 11]] \
accuracy: 0.8703703703703703 \
precision: 0.866971916971917 \
f1 score: 0.8646943691659139 \
nastÄ…piÅ‚ juÅ¼ duÅ¼y spadek wydajnoÅ›ci wzglÄ™dem wiÄ™kszego modelu 

ðŸ“Œ Model 6 â€” 36 kb\
Layer (type)                 Output Shape     Param # \
dense (Dense)                (None, 12)       168     \
activation (Activation)      (None, 12)       0       \
dense_1 (Dense)              (None, 6)        78      \
layer_normalization          (None, 6)        12      \
activation_1 (Activation)    (None, 6)        0       \
dense_2 (Dense)              (None, 3)        21      

confusion matrix: \
[[19 0 0] \
 [ 0 21 0] \
 [ 1 0 13]] \
accuracy: 0.9814814814814815 \
precision: 0.9833333333333334 \
f1 score: 0.9791073124406457 \
model tego rozmiaru poradziÅ‚ sobie bardzo dobrze, choÄ‡ wpadÅ‚ w minimum \
lokalne gdzie accuracy wynosiÅ‚o +- 0.65 w trakcie epok 25-75, jednak kaÅ¼dy \
model uczony byÅ‚ na 200 epokach i finalny wynik jest zadowalajÄ…cy 

ðŸ“Œ Model 7 â€” 37 kb\
Layer (type)                 Output Shape     Param # \
dense (Dense)                (None, 10)       140     \
activation (Activation)      (None, 10)       0       \
dense_1 (Dense)              (None, 5)        55      \
layer_normalization          (None, 5)        10      \
activation_1 (Activation)    (None, 5)        0       \
dense_2 (Dense)              (None, 3)        18      

confusion matrix: \
[[19 0 0] \
 [ 0 19 2] \
 [ 0 12 2]] \
accuracy: 0.7407407407407407 \
precision: 0.7043010752688171 \
f1 score: 0.650997150997151 \
ten model poradziÅ‚ sobie bardzo Åºle w porÃ³wnaniu z bazowym 

ðŸ“Œ Model 8 â€” z Dropout â€” 36 kb\
Layer (type)                 Output Shape     Param # \
dense (Dense)                (None, 10)       140     \
activation (Activation)      (None, 10)       0       \
dropout (Dropout)            (None, 10)       0       \
dense_1 (Dense)              (None, 5)        55      \
activation_1 (Activation)    (None, 5)        0       \
dropout_1 (Dropout)          (None, 5)        0       \
dense_2 (Dense)              (None, 3)        18      

confusion matrix: \
[[17 2 0] \
 [ 0 21 0] \
 [ 0 14 0]] \
accuracy: 0.7037037037037037 \
precision: 0.5225225225225225 \
f1 score: 0.5561941251596424 \
model z warstwami dropout wypadÅ‚ bardzo Åºle na tle modelu bazowego 

ðŸ“Œ Model 9 â€” L2 Regularization â€” 38 kb\
Layer (type)                 Output Shape     Param # \
dense (Dense)                (None, 32)       448     \
dense_1 (Dense)              (None, 16)       528     \
dense_2 (Dense)              (None, 3)        51      

confusion matrix: \
[[19 0 0] \
 [ 7 14 0] \
 [ 0 0 14]] \
accuracy: 0.8703703703703703 \
precision: 0.9102564102564102 \
f1 score: 0.8814814814814814 \
model z regularyzacjÄ… l2 wypadÅ‚ podobnie co bazowy 

ðŸ“Œ Wnioski

Najmniejszym i jednoczeÅ›nie dobrze sprawdzajÄ…cym siÄ™ modelem jest:

Layer (type)                 Output Shape     Param # \
dense (Dense)                (None, 12)       168     \
activation (Activation)      (None, 12)       0       \
dense_1 (Dense)              (None, 6)        78      \
layer_normalization          (None, 6)        12      \
activation_1 (Activation)    (None, 6)        0       \
dense_2 (Dense)              (None, 3)        21      


RÃ³Å¼nica rozmiaru w porÃ³wnaniu z modelem bazowym: 4 kb \